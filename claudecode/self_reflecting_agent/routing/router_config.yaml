# Model Router Configuration
# Defines available models, their capabilities, and routing preferences
#
# MULTI-KEY FAILOVER SUPPORT:
# This router supports multiple API keys per provider for automatic failover.
# Set multiple keys in .env file using pattern: PROVIDER_API_KEY, PROVIDER_API_KEY_2, etc.
# The system will automatically failover when keys hit rate limits or errors.
#
# Supported providers with multi-key failover:
# - ANTHROPIC_API_KEY, ANTHROPIC_API_KEY_2
# - OPENAI_API_KEY, OPENAI_API_KEY_2  
# - GOOGLE_API_KEY, GOOGLE_API_KEY_2
# - OPENROUTER_API_KEY, OPENROUTER_API_KEY_2
# - GROQ_API_KEY, GROQ_API_KEY_2

models:
  # Anthropic Claude Models
  claude-3-5-sonnet-20241022:
    provider: anthropic
    api_key_env: ANTHROPIC_API_KEY
    base_url: null
    capabilities:
      context_length: 200000
      supports_code: true
      supports_reasoning: true
      supports_vision: true
      supports_function_calling: true
      latency_ms: 3000
      cost_per_1k_tokens: 0.003
      quality_score: 0.95
    model_params:
      max_tokens: 8192
      temperature: 0.1
    priority: 10
    enabled: true
    
  claude-3-5-haiku-20241022:
    provider: anthropic
    api_key_env: ANTHROPIC_API_KEY
    base_url: null
    capabilities:
      context_length: 200000
      supports_code: true
      supports_reasoning: true
      supports_vision: false
      supports_function_calling: true
      latency_ms: 1500
      cost_per_1k_tokens: 0.001
      quality_score: 0.88
    model_params:
      max_tokens: 8192
      temperature: 0.1
    priority: 6
    enabled: true
    
  claude-3-haiku-20240307:
    provider: anthropic
    api_key_env: ANTHROPIC_API_KEY
    base_url: null
    capabilities:
      context_length: 200000
      supports_code: true
      supports_reasoning: true
      supports_vision: false
      supports_function_calling: true
      latency_ms: 1800
      cost_per_1k_tokens: 0.00025
      quality_score: 0.82
    model_params:
      max_tokens: 4096
      temperature: 0.1
    priority: 3
    enabled: true

  # OpenAI GPT Models
  gpt-4o:
    provider: openai
    api_key_env: OPENAI_API_KEY
    base_url: null
    capabilities:
      context_length: 128000
      supports_code: true
      supports_reasoning: true
      supports_vision: true
      supports_function_calling: true
      latency_ms: 4000
      cost_per_1k_tokens: 0.005
      quality_score: 0.92
    model_params:
      max_tokens: 4096
      temperature: 0.1
    priority: 8
    enabled: true
    
  gpt-4o-mini:
    provider: openai
    api_key_env: OPENAI_API_KEY
    base_url: null
    capabilities:
      context_length: 128000
      supports_code: true
      supports_reasoning: true
      supports_vision: true
      supports_function_calling: true
      latency_ms: 2000
      cost_per_1k_tokens: 0.00015
      quality_score: 0.8
    model_params:
      max_tokens: 4096
      temperature: 0.1
    priority: 4
    enabled: true
    
  gpt-4-turbo:
    provider: openai
    api_key_env: OPENAI_API_KEY
    base_url: null
    capabilities:
      context_length: 128000
      supports_code: true
      supports_reasoning: true
      supports_vision: true
      supports_function_calling: true
      latency_ms: 5000
      cost_per_1k_tokens: 0.01
      quality_score: 0.94
    model_params:
      max_tokens: 4096
      temperature: 0.1
    priority: 9
    enabled: false  # Expensive, enable if needed

  # Google Gemini Models  
  gemini-2.0-flash-exp:
    provider: google
    api_key_env: GOOGLE_API_KEY
    base_url: null
    capabilities:
      context_length: 1000000
      supports_code: true
      supports_reasoning: true
      supports_vision: true
      supports_function_calling: true
      latency_ms: 2500
      cost_per_1k_tokens: 0.00075
      quality_score: 0.90
    model_params:
      max_output_tokens: 8192
      temperature: 0.1
    priority: 7
    enabled: true
    
  gemini-1.5-pro:
    provider: google
    api_key_env: GOOGLE_API_KEY
    base_url: null
    capabilities:
      context_length: 2000000
      supports_code: true
      supports_reasoning: true
      supports_vision: true
      supports_function_calling: true
      latency_ms: 6000
      cost_per_1k_tokens: 0.0035
      quality_score: 0.91
    model_params:
      max_output_tokens: 8192
      temperature: 0.1
    priority: 5
    enabled: true
    
  gemini-1.5-flash:
    provider: google
    api_key_env: GOOGLE_API_KEY
    base_url: null
    capabilities:
      context_length: 1000000
      supports_code: true
      supports_reasoning: true
      supports_vision: true
      supports_function_calling: true
      latency_ms: 1200
      cost_per_1k_tokens: 0.00015
      quality_score: 0.83
    model_params:
      max_output_tokens: 8192
      temperature: 0.1
    priority: 2
    enabled: true

  # OpenRouter Models - Access to diverse model ecosystem
  anthropic/claude-3-5-sonnet:
    provider: openrouter
    api_key_env: OPENROUTER_API_KEY
    base_url: https://openrouter.ai/api/v1
    capabilities:
      context_length: 200000
      supports_code: true
      supports_reasoning: true
      supports_vision: true
      supports_function_calling: true
      latency_ms: 3500
      cost_per_1k_tokens: 0.003
      quality_score: 0.95
    model_params:
      max_tokens: 8192
      temperature: 0.1
    priority: 11
    enabled: true
    
  openai/gpt-4o:
    provider: openrouter
    api_key_env: OPENROUTER_API_KEY
    base_url: https://openrouter.ai/api/v1
    capabilities:
      context_length: 128000
      supports_code: true
      supports_reasoning: true
      supports_vision: true
      supports_function_calling: true
      latency_ms: 4500
      cost_per_1k_tokens: 0.005
      quality_score: 0.92
    model_params:
      max_tokens: 4096
      temperature: 0.1
    priority: 9
    enabled: true
    
  google/gemini-pro-1.5:
    provider: openrouter
    api_key_env: OPENROUTER_API_KEY
    base_url: https://openrouter.ai/api/v1
    capabilities:
      context_length: 2000000
      supports_code: true
      supports_reasoning: true
      supports_vision: true
      supports_function_calling: true
      latency_ms: 5000
      cost_per_1k_tokens: 0.00125
      quality_score: 0.90
    model_params:
      max_tokens: 8192
      temperature: 0.1
    priority: 7
    enabled: true
    
  meta-llama/llama-3.1-405b-instruct:
    provider: openrouter
    api_key_env: OPENROUTER_API_KEY
    base_url: https://openrouter.ai/api/v1
    capabilities:
      context_length: 131072
      supports_code: true
      supports_reasoning: true
      supports_vision: false
      supports_function_calling: true
      latency_ms: 8000
      cost_per_1k_tokens: 0.009
      quality_score: 0.91
    model_params:
      max_tokens: 4096
      temperature: 0.1
    priority: 8
    enabled: true
    
  anthropic/claude-3-haiku:
    provider: openrouter
    api_key_env: OPENROUTER_API_KEY
    base_url: https://openrouter.ai/api/v1
    capabilities:
      context_length: 200000
      supports_code: true
      supports_reasoning: true
      supports_vision: false
      supports_function_calling: true
      latency_ms: 2000
      cost_per_1k_tokens: 0.00025
      quality_score: 0.85
    model_params:
      max_tokens: 4096
      temperature: 0.1
    priority: 4
    enabled: true

  # Groq Models - Ultra-fast inference
  llama-3.3-70b-versatile:
    provider: groq
    api_key_env: GROQ_API_KEY
    base_url: https://api.groq.com/openai/v1
    capabilities:
      context_length: 128000
      supports_code: true
      supports_reasoning: true
      supports_vision: false
      supports_function_calling: true
      latency_ms: 300  # Ultra-fast
      cost_per_1k_tokens: 0.00059
      quality_score: 0.89
    model_params:
      max_tokens: 8192
      temperature: 0.1
    priority: 10
    enabled: true
    
  llama3-70b-8192:
    provider: groq
    api_key_env: GROQ_API_KEY
    base_url: https://api.groq.com/openai/v1
    capabilities:
      context_length: 8192
      supports_code: true
      supports_reasoning: true
      supports_vision: false
      supports_function_calling: true
      latency_ms: 250  # Ultra-fast
      cost_per_1k_tokens: 0.00059
      quality_score: 0.88
    model_params:
      max_tokens: 8192
      temperature: 0.1
    priority: 9
    enabled: true
    
  mixtral-8x7b-32768:
    provider: groq
    api_key_env: GROQ_API_KEY
    base_url: https://api.groq.com/openai/v1
    capabilities:
      context_length: 32768
      supports_code: true
      supports_reasoning: true
      supports_vision: false
      supports_function_calling: true
      latency_ms: 200  # Ultra-fast
      cost_per_1k_tokens: 0.00024
      quality_score: 0.85
    model_params:
      max_tokens: 32768
      temperature: 0.1
    priority: 7
    enabled: true
    
  llama3-8b-8192:
    provider: groq
    api_key_env: GROQ_API_KEY
    base_url: https://api.groq.com/openai/v1
    capabilities:
      context_length: 8192
      supports_code: true
      supports_reasoning: true
      supports_vision: false
      supports_function_calling: true
      latency_ms: 150  # Ultra-fast
      cost_per_1k_tokens: 0.00005
      quality_score: 0.80
    model_params:
      max_tokens: 8192
      temperature: 0.1
    priority: 5
    enabled: true
    
  gemma2-9b-it:
    provider: groq
    api_key_env: GROQ_API_KEY
    base_url: https://api.groq.com/openai/v1
    capabilities:
      context_length: 8192
      supports_code: true
      supports_reasoning: true
      supports_vision: false
      supports_function_calling: true
      latency_ms: 180  # Ultra-fast
      cost_per_1k_tokens: 0.00020
      quality_score: 0.82
    model_params:
      max_tokens: 8192
      temperature: 0.1
    priority: 6
    enabled: true

# Routing Rules - Define preferred models for each task type
# Models are listed in order of preference
routing_rules:
  # High-level orchestration and planning
  orchestration:
    - anthropic/claude-3-5-sonnet  # Best reasoning through OpenRouter
    - claude-3-5-sonnet-20241022   # Best reasoning and planning
    - meta-llama/llama-3.1-405b-instruct  # Large model via OpenRouter
    - gpt-4o                       # Strong general capability
    - gemini-2.0-flash-exp         # Good reasoning, large context
    
  # Code generation tasks
  code_generation:
    - anthropic/claude-3-5-sonnet  # Excellent code quality via OpenRouter
    - claude-3-5-sonnet-20241022   # Excellent code quality
    - openai/gpt-4o                # Strong coding via OpenRouter
    - gpt-4o                       # Strong coding ability
    - gemini-2.0-flash-exp         # Good code generation
    
  # Debugging and error analysis
  debugging:
    - llama-3.3-70b-versatile      # Ultra-fast debugging iterations via Groq
    - google/gemini-pro-1.5        # Large context debugging via OpenRouter
    - gemini-2.0-flash-exp         # Optimized for debugging
    - anthropic/claude-3-5-sonnet  # Great analysis via OpenRouter
    - claude-3-5-sonnet-20241022   # Great analysis capability
    
  # Code review and analysis
  code_review:
    - anthropic/claude-3-5-sonnet  # Best analysis via OpenRouter
    - claude-3-5-sonnet-20241022   # Best at thorough analysis
    - openai/gpt-4o                # Strong review via OpenRouter
    - gpt-4o                       # Strong review capability
    - gemini-1.5-pro               # Good for large codebases
    
  # System architecture and design
  architecture:
    - anthropic/claude-3-5-sonnet  # Excellent system thinking via OpenRouter
    - claude-3-5-sonnet-20241022   # Excellent system thinking
    - meta-llama/llama-3.1-405b-instruct  # Large model reasoning via OpenRouter
    - gpt-4-turbo                  # Strong architectural planning
    - google/gemini-pro-1.5        # Large context via OpenRouter
    
  # Documentation writing
  documentation:
    - anthropic/claude-3-haiku     # Cost-effective via OpenRouter
    - claude-3-haiku-20240307      # Cost-effective, good quality
    - gpt-4o-mini                  # Cheap and capable
    - gemini-1.5-flash             # Fast documentation
    - claude-3-5-haiku-20241022    # Higher quality when needed
    
  # Test creation and testing
  testing:
    - llama3-70b-8192              # Fast test generation via Groq
    - claude-3-5-sonnet-20241022   # Thorough test creation
    - gpt-4o                       # Good test coverage
    - gemini-2.0-flash-exp         # Good testing patterns
    - claude-3-5-haiku-20241022    # Quick test generation
    
  # Code refactoring
  refactoring:
    - claude-3-5-sonnet-20241022   # Best at understanding intent
    - gpt-4o                       # Good refactoring capability
    - gemini-1.5-pro               # Large context for big refactors
    - claude-3-5-haiku-20241022    # Quick refactoring
    
  # Research and information gathering
  research:
    - gemini-1.5-pro               # Large context for research
    - claude-3-5-sonnet-20241022   # Thorough research capability
    - gpt-4o                       # Good research skills
    - gemini-2.0-flash-exp         # Good information synthesis
    
  # Data and code analysis
  analysis:
    - claude-3-5-sonnet-20241022   # Excellent analytical thinking
    - gemini-1.5-pro               # Large context for analysis
    - gpt-4o                       # Strong analysis capability
    - gemini-2.0-flash-exp         # Good analytical capability
    
  # General conversation and interaction
  conversation:
    - llama3-8b-8192               # Ultra-fast responses via Groq
    - gemma2-9b-it                 # Fast and affordable via Groq
    - claude-3-haiku-20240307      # Cost-effective conversation
    - gpt-4o-mini                  # Cheap general interaction
    - gemini-1.5-flash             # Fast responses
    
  # Complex reasoning tasks
  reasoning:
    - llama-3.3-70b-versatile      # Fast high-quality reasoning via Groq
    - meta-llama/llama-3.1-405b-instruct  # Large model reasoning via OpenRouter
    - anthropic/claude-3-5-sonnet  # Best reasoning via OpenRouter
    - claude-3-5-sonnet-20241022   # Best reasoning capability
    - google/gemini-pro-1.5        # Large context reasoning via OpenRouter

# Fallback Configuration
fallback:
  # Maximum number of fallback attempts
  max_attempts: 3
  
  # Delay between fallback attempts (seconds)
  retry_delay: 2
  
  # Exponential backoff multiplier
  backoff_multiplier: 2.0
  
  # Emergency fallback models (always try these if configured models fail)
  emergency_models:
    - claude-3-haiku-20240307
    - gpt-4o-mini
    - gemini-1.5-flash

# Performance Monitoring
monitoring:
  # Track performance metrics
  enable_performance_tracking: true
  
  # Minimum requests before using performance data for routing
  min_requests_for_performance: 5
  
  # Weight of performance vs capability in routing decisions
  performance_weight: 0.4
  capability_weight: 0.4
  priority_weight: 0.2
  
  # Performance thresholds
  min_success_rate: 0.7
  max_acceptable_latency_ms: 30000
  
  # Availability check intervals
  availability_cache_duration: 300  # 5 minutes

# Cost Management
cost_management:
  # Enable cost-aware routing
  enable_cost_optimization: true
  
  # Daily cost limits per model (USD)
  daily_limits:
    gpt-4-turbo: 10.0
    claude-3-5-sonnet-20241022: 15.0
    gemini-1.5-pro: 8.0
  
  # Cost-sensitive task types (will prefer cheaper models)
  cost_sensitive_tasks:
    - documentation
    - conversation
    - testing
  
  # Alert thresholds
  cost_alert_threshold: 50.0  # Alert when daily cost exceeds this

# Rate Limiting
rate_limiting:
  # Enable rate limit management
  enable_rate_limiting: true
  
  # Default rate limits (requests per minute)
  default_rate_limits:
    anthropic: 60
    openai: 60
    google: 60
    openrouter: 200
    groq: 500  # Much higher rate limits
  
  # Model-specific rate limits (override defaults)
  model_rate_limits:
    gpt-4-turbo: 20
    claude-3-5-sonnet-20241022: 50

# Context Management Settings
context_management:
  # Enable context preservation across model switches
  preserve_context: true
  
  # Maximum context to preserve when switching models
  max_preserved_context: 50000
  
  # Context compression strategy when switching to smaller context models
  compression_strategy: "summarize"  # "summarize" or "truncate"
  
  # Include conversation history in context preservation
  include_conversation_history: true
  
  # Maximum conversation turns to preserve
  max_conversation_turns: 10